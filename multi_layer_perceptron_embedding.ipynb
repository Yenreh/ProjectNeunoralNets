{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "493885d5",
   "metadata": {},
   "source": [
    "# Proyecto Redes Neuronales: Perceptrón Multicapa\n",
    "\n",
    "**Curso:** Redes Neuronales 2025-II  \n",
    "**Objetivo:** Entrenar y evaluar un modelo de Perceptrón Multicapa (MLP) para clasificación de texto según el enunciado del proyecto.  \n",
    "**Autor:** Herney Eduardo Quintero Trochez  \n",
    "**Fecha:** 2025  \n",
    "**Universidad:** Universidad Del Valle  \n",
    "\n",
    "## Componentes implementados:\n",
    "1. Configuración de Parámetros Globales\n",
    "2. Carga y Preprocesamiento de Datos\n",
    "3. Tokenización y Creación del Vocabulario\n",
    "4. Construcción del Modelo MLP\n",
    "5. Entrenamiento con Early Stopping\n",
    "6. Evaluación del Modelo\n",
    "7. Guardado de Resultados y Modelos\n",
    "8. Visualización de Resultados\n",
    "9. Historial de Experimentos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f37abf",
   "metadata": {},
   "source": [
    "## 0. Configuración de Parámetros Globales\n",
    "\n",
    "Esta sección permite modificar fácilmente todos los parámetros del modelo para experimentar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b42a10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CONFIGURACIÓN DE PARÁMETROS GLOBALES =====\n",
    "# Esta sección centraliza todos los parámetros del modelo para facilitar experimentación\n",
    "\n",
    "# Configuración general del experimento\n",
    "EXPERIMENT_NAME = \"MultiLayer_Perceptron (Embedding)\"  # Nombre descriptivo del experimento\n",
    "MODEL_TYPE = \"MLP Embedding\"  # Perceptrón Multi-Capa  \n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Directorios de trabajo\n",
    "DATA_DIR = \"data\"\n",
    "MODEL_DIR = \"models\"\n",
    "OUTPUT_DIR = \"output\"\n",
    "\n",
    "# Configuración del dataset - MEJORADO CON TÍTULO\n",
    "TEXT_COLUMN = \"review_body\"  # Columna con el texto del cuerpo de la reseña\n",
    "TITLE_COLUMN = \"review_title\"  # Columna con el título de la reseña\n",
    "TARGET_COLUMN = \"stars\"  # Columna con las etiquetas (1-5 estrellas)\n",
    "LANGUAGE_COLUMN = \"language\"  # Columna con el idioma\n",
    "FILTER_LANGUAGE = \"en\"  # Filtrar por idioma específico. Opciones: None, \"en\", \"es\", \"de\", \"fr\", \"ja\"\n",
    "USE_TITLE_AND_BODY = True  # NUEVO: Usar tanto título como cuerpo para mejor precisión\n",
    "MAX_WORDS = 80000  # Tamaño máximo del vocabulario\n",
    "MAX_LENGTH = 300  # Longitud máxima de las secuencias (aumentada por combinar título+cuerpo)\n",
    "OOV_TOKEN = \"<OOV>\"  # Token para palabras fuera del vocabulario\n",
    "\n",
    "# Parámetros de arquitectura del modelo - MLP\n",
    "EMBEDDING_DIM = 300  # Dimensión del embedding de palabras\n",
    "HIDDEN_LAYERS = [256, 128, 64]  # Lista con el número de neuronas en cada capa oculta\n",
    "ACTIVATION = \"relu\"  # Función de activación para capas ocultas\n",
    "OUTPUT_ACTIVATION = \"softmax\"  # Función de activación para la capa de salida\n",
    "DROPOUT_RATE = 0.3  # Tasa de dropout para regularización\n",
    "\n",
    "# Parámetros de entrenamiento\n",
    "EPOCHS = 50  # Número máximo de épocas de entrenamiento\n",
    "BATCH_SIZE = 512  # Tamaño del batch\n",
    "LEARNING_RATE = 0.001  # Tasa de aprendizaje\n",
    "PATIENCE = 10  # Paciencia para early stopping\n",
    "OPTIMIZER = \"adam\"  # Optimizador a usar\n",
    "LOSS_FUNCTION = \"categorical_crossentropy\"  # Función de pérdida\n",
    "METRICS = [\"accuracy\"]  # Métricas a monitorear\n",
    "\n",
    "print(f\"=== Configuración del Experimento: {EXPERIMENT_NAME} ===\")\n",
    "print(f\"Modelo: {MODEL_TYPE}\")\n",
    "print(f\"Filtro de idioma: {FILTER_LANGUAGE if FILTER_LANGUAGE else 'Multiidioma'}\")\n",
    "print(f\"Usar título + cuerpo: {USE_TITLE_AND_BODY}\")\n",
    "print(f\"Longitud máxima: {MAX_LENGTH} tokens\")\n",
    "print(f\"Arquitectura oculta: {HIDDEN_LAYERS}\")\n",
    "print(f\"Dimensión embedding: {EMBEDDING_DIM}\")\n",
    "print(f\"Dropout: {DROPOUT_RATE}\")\n",
    "print(f\"Épocas máximas: {EPOCHS}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Tasa de aprendizaje: {LEARNING_RATE}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6ffb44",
   "metadata": {},
   "source": [
    "## 1. Importación de Librerías y Funciones Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f08898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== IMPORTAR LIBRERÍAS Y MÓDULOS =====\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Importar funciones helper\n",
    "from helper import (\n",
    "    DataLoader, ModelTrainer, ResultsManager, Visualizer,\n",
    "    evaluate_model, setup_experiment_environment\n",
    ")\n",
    "\n",
    "# Configurar ambiente del experimento\n",
    "gpu_info = setup_experiment_environment(RANDOM_SEED)\n",
    "print(f\"Ambiente configurado. GPU disponible: {gpu_info['gpu_available']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbf34a3",
   "metadata": {},
   "source": [
    "## 2. Carga y Preprocesamiento de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e63e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar el cargador de datos\n",
    "data_loader = DataLoader(data_dir=DATA_DIR)\n",
    "\n",
    "# Cargar los datasets\n",
    "print(\"Cargando datasets...\")\n",
    "train_df, val_df, test_df = data_loader.load_all_data()\n",
    "\n",
    "print(f\"\\nDatos originales cargados:\")\n",
    "print(f\"Entrenamiento: {len(train_df)} muestras\")\n",
    "print(f\"Validación: {len(val_df)} muestras\") \n",
    "print(f\"Prueba: {len(test_df)} muestras\")\n",
    "\n",
    "# Verificar que las columnas necesarias existen\n",
    "required_columns = [TEXT_COLUMN, TARGET_COLUMN, LANGUAGE_COLUMN]\n",
    "if USE_TITLE_AND_BODY:\n",
    "    required_columns.append(TITLE_COLUMN)\n",
    "\n",
    "missing_columns = [col for col in required_columns if col not in train_df.columns]\n",
    "if missing_columns:\n",
    "    print(f\"Columnas faltantes: {missing_columns}\")\n",
    "    print(f\"Columnas disponibles: {list(train_df.columns)}\")\n",
    "else:\n",
    "    print(f\"Todas las columnas requeridas están disponibles\")\n",
    "    if USE_TITLE_AND_BODY:\n",
    "        print(f\"Modo combinado: {TITLE_COLUMN} + {TEXT_COLUMN}\")\n",
    "\n",
    "# Analizar distribución de idiomas\n",
    "print(f\"\\nAnálisis de idiomas en el dataset:\")\n",
    "if LANGUAGE_COLUMN in train_df.columns:\n",
    "    lang_dist_train = train_df[LANGUAGE_COLUMN].value_counts()\n",
    "    print(f\"Distribución de idiomas (entrenamiento):\")\n",
    "    for lang, count in lang_dist_train.items():\n",
    "        percentage = (count / len(train_df)) * 100\n",
    "        print(f\"  {lang}: {count:,} muestras ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Aplicar filtro por idioma si está especificado\n",
    "    if FILTER_LANGUAGE is not None:\n",
    "        if FILTER_LANGUAGE in lang_dist_train.index:\n",
    "            print(f\"\\nFiltrando por idioma: {FILTER_LANGUAGE}\")\n",
    "            \n",
    "            # Filtrar datasets por idioma\n",
    "            train_df = train_df[train_df[LANGUAGE_COLUMN] == FILTER_LANGUAGE].copy()\n",
    "            val_df = val_df[val_df[LANGUAGE_COLUMN] == FILTER_LANGUAGE].copy()\n",
    "            test_df = test_df[test_df[LANGUAGE_COLUMN] == FILTER_LANGUAGE].copy()\n",
    "            \n",
    "            print(f\"\\nDatos después del filtrado por idioma '{FILTER_LANGUAGE}':\")\n",
    "            print(f\"Entrenamiento: {len(train_df)} muestras\")\n",
    "            print(f\"Validación: {len(val_df)} muestras\")\n",
    "            print(f\"Prueba: {len(test_df)} muestras\")\n",
    "        else:\n",
    "            print(f\"\\nAdvertencia: Idioma '{FILTER_LANGUAGE}' no encontrado en el dataset.\")\n",
    "            print(f\"Idiomas disponibles: {list(lang_dist_train.index)}\")\n",
    "            print(\"Usando todos los idiomas...\")\n",
    "    else:\n",
    "        print(f\"\\nUsando todos los idiomas disponibles\")\n",
    "else:\n",
    "    print(f\"Columna '{LANGUAGE_COLUMN}' no encontrada. Usando todos los datos sin filtrar.\")\n",
    "\n",
    "# Mostrar distribución de clases en el conjunto final\n",
    "print(f\"\\nDistribución de clases (conjunto final):\")\n",
    "class_distribution = train_df[TARGET_COLUMN].value_counts().sort_index()\n",
    "for stars, count in class_distribution.items():\n",
    "    percentage = (count / len(train_df)) * 100\n",
    "    print(f\"  {stars} estrella(s): {count:,} muestras ({percentage:.1f}%)\")\n",
    "\n",
    "# Mostrar ejemplos de datos con título y cuerpo\n",
    "print(f\"\\nEjemplos de datos del conjunto final:\")\n",
    "for i in range(min(3, len(train_df))):\n",
    "    lang = train_df[LANGUAGE_COLUMN].iloc[i] if LANGUAGE_COLUMN in train_df.columns else \"N/A\"\n",
    "    title = train_df[TITLE_COLUMN].iloc[i] if USE_TITLE_AND_BODY and TITLE_COLUMN in train_df.columns else \"N/A\"\n",
    "    text = train_df[TEXT_COLUMN].iloc[i][:80]  # Menos texto para mostrar título también\n",
    "    stars = train_df[TARGET_COLUMN].iloc[i]\n",
    "    \n",
    "    print(f\"{i+1}. [{lang}] {stars} estrella(s)\")\n",
    "    if USE_TITLE_AND_BODY and title != \"N/A\":\n",
    "        print(f\"   Título: {title[:60]}{'...' if len(str(title)) > 60 else ''}\")\n",
    "    print(f\"   Cuerpo: {text}{'...' if len(str(train_df[TEXT_COLUMN].iloc[i])) > 80 else ''}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26214504",
   "metadata": {},
   "source": [
    "## 3. Preprocesamiento de Texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec025b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesar los datos de texto\n",
    "print(\"Preprocesando datos de texto...\")\n",
    "print(f\"Modo: {'Título + Cuerpo' if USE_TITLE_AND_BODY else 'Solo Cuerpo'}\")\n",
    "\n",
    "processed_data = data_loader.preprocess_text_data_embedding(\n",
    "    train_df=train_df,\n",
    "    val_df=val_df, \n",
    "    test_df=test_df,\n",
    "    text_column=TEXT_COLUMN,\n",
    "    title_column=TITLE_COLUMN if USE_TITLE_AND_BODY else None,\n",
    "    target_column=TARGET_COLUMN,\n",
    "    max_words=MAX_WORDS,\n",
    "    max_length=MAX_LENGTH,\n",
    "    use_title_and_body=USE_TITLE_AND_BODY\n",
    ")\n",
    "\n",
    "# Extraer datos preprocesados\n",
    "X_train, y_train = processed_data['X_train'], processed_data['y_train']\n",
    "X_val, y_val = processed_data['X_val'], processed_data['y_val']\n",
    "X_test, y_test = processed_data['X_test'], processed_data['y_test']\n",
    "num_classes = processed_data['num_classes']\n",
    "vocab_size = processed_data['vocab_size']\n",
    "\n",
    "print(f\"\\nDatos preprocesados:\")\n",
    "print(f\"Tamaño del vocabulario: {vocab_size}\")\n",
    "print(f\"Número de clases: {num_classes}\")\n",
    "print(f\"Forma de X_train: {X_train.shape}\")\n",
    "print(f\"Forma de y_train: {y_train.shape}\")\n",
    "print(f\"Texto combinado: {'Sí (título + cuerpo)' if USE_TITLE_AND_BODY else 'No (solo cuerpo)'}\")\n",
    "\n",
    "# Obtener nombres de clases para evaluación\n",
    "class_names = [str(i) for i in data_loader.label_encoder.classes_]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a4f8d1",
   "metadata": {},
   "source": [
    "## 4. Construcción del Modelo MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cf91a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp_model(vocab_size, embedding_dim, max_length, hidden_layers, \n",
    "                     num_classes, dropout_rate, activation, output_activation):\n",
    "    \"\"\"\n",
    "    Crear un modelo de Perceptrón Multicapa para clasificación de texto.\n",
    "    \n",
    "    Args:\n",
    "        vocab_size: Tamaño del vocabulario\n",
    "        embedding_dim: Dimensión del embedding\n",
    "        max_length: Longitud máxima de secuencia\n",
    "        hidden_layers: Lista con el número de neuronas en cada capa oculta\n",
    "        num_classes: Número de clases de salida\n",
    "        dropout_rate: Tasa de dropout\n",
    "        activation: Función de activación para capas ocultas\n",
    "        output_activation: Función de activación para la capa de salida\n",
    "    \n",
    "    Returns:\n",
    "        tf.keras.Model: Modelo compilado\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Capa de embedding (sin input_length que está deprecado)\n",
    "    model.add(Embedding(input_dim=vocab_size, \n",
    "                       output_dim=embedding_dim,\n",
    "                       name=\"embedding_layer\"))\n",
    "    \n",
    "    # Pooling global para reducir dimensionalidad\n",
    "    model.add(GlobalAveragePooling1D(name=\"global_avg_pooling\"))\n",
    "    \n",
    "    # Capas ocultas del MLP\n",
    "    for i, units in enumerate(hidden_layers):\n",
    "        model.add(Dense(units=units, \n",
    "                       activation=activation, \n",
    "                       name=f\"dense_layer_{i+1}\"))\n",
    "        model.add(Dropout(rate=dropout_rate, \n",
    "                         name=f\"dropout_{i+1}\"))\n",
    "    \n",
    "    # Capa de salida\n",
    "    model.add(Dense(units=num_classes, \n",
    "                   activation=output_activation, \n",
    "                   name=\"output_layer\"))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Crear el modelo\n",
    "print(\"Creando modelo MLP...\")\n",
    "model = create_mlp_model(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    max_length=MAX_LENGTH,\n",
    "    hidden_layers=HIDDEN_LAYERS,\n",
    "    num_classes=num_classes,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    activation=ACTIVATION,\n",
    "    output_activation=OUTPUT_ACTIVATION\n",
    ")\n",
    "\n",
    "# Compilar el modelo\n",
    "optimizer = Adam(learning_rate=LEARNING_RATE)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=LOSS_FUNCTION,\n",
    "    metrics=METRICS\n",
    ")\n",
    "\n",
    "# Construir el modelo con la forma de entrada específica\n",
    "model.build(input_shape=(None, MAX_LENGTH))\n",
    "\n",
    "# Mostrar arquitectura del modelo\n",
    "print(\"\\nArquitectura del modelo:\")\n",
    "model.summary()\n",
    "\n",
    "# Contar parámetros\n",
    "total_params = model.count_params()\n",
    "print(f\"\\nTotal de parámetros: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e33caa8",
   "metadata": {},
   "source": [
    "## 5. Entrenamiento del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071b8782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar el entrenador de modelos\n",
    "model_trainer = ModelTrainer(model_dir=MODEL_DIR)\n",
    "\n",
    "# Entrenar el modelo\n",
    "print(\"Iniciando entrenamiento...\")\n",
    "training_results = model_trainer.train_model(\n",
    "    model=model,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    patience=PATIENCE,\n",
    "    model_name=f\"{MODEL_TYPE}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    ")\n",
    "\n",
    "print(f\"\\nEntrenamiento completado:\")\n",
    "print(f\"Épocas entrenadas: {training_results['epochs_trained']}\")\n",
    "print(f\"Tiempo de entrenamiento: {training_results['training_time']:.1f} segundos\")\n",
    "print(f\"Accuracy final (entrenamiento): {training_results['final_train_accuracy']:.4f}\")\n",
    "print(f\"Accuracy final (validación): {training_results['final_val_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24905dfc",
   "metadata": {},
   "source": [
    "## 6. Visualización del Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c99001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar el historial de entrenamiento\n",
    "Visualizer.plot_training_history(\n",
    "    history=training_results['history'],\n",
    "    model_name=MODEL_TYPE,\n",
    "    save_path=os.path.join(OUTPUT_DIR, f\"{MODEL_TYPE}_training_history.png\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4484364",
   "metadata": {},
   "source": [
    "## 7. Evaluación del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3f6730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar el modelo en el conjunto de prueba\n",
    "evaluation_results = evaluate_model(\n",
    "    model=model,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    class_names=class_names\n",
    ")\n",
    "\n",
    "# Extraer métricas de evaluación\n",
    "test_accuracy = evaluation_results['test_accuracy']\n",
    "test_loss = evaluation_results['test_loss']\n",
    "classification_rep = evaluation_results['classification_report']\n",
    "y_true = evaluation_results['y_true']\n",
    "y_pred = evaluation_results['y_pred']\n",
    "\n",
    "print(f\"\\nResultados en conjunto de prueba:\")\n",
    "print(f\"Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Loss: {test_loss:.4f}\")\n",
    "print(f\"F1-Score (macro): {classification_rep['macro avg']['f1-score']:.4f}\")\n",
    "print(f\"F1-Score (weighted): {classification_rep['weighted avg']['f1-score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc34bb6f",
   "metadata": {},
   "source": [
    "## 8. Matriz de Confusión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4183361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar matriz de confusión\n",
    "Visualizer.plot_confusion_matrix(\n",
    "    y_true=y_true,\n",
    "    y_pred=y_pred,\n",
    "    class_names=class_names,\n",
    "    model_name=MODEL_TYPE,\n",
    "    save_path=os.path.join(OUTPUT_DIR, f\"{MODEL_TYPE}_confusion_matrix.png\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bae31f",
   "metadata": {},
   "source": [
    "## 9. Guardado de Resultados del Experimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625c28f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar datos del experimento para guardar\n",
    "experiment_data = {\n",
    "    'experiment_name': EXPERIMENT_NAME,\n",
    "    'configuration': {\n",
    "        'model_type': MODEL_TYPE,\n",
    "        'text_column': TEXT_COLUMN,\n",
    "        'target_column': TARGET_COLUMN,\n",
    "        'language_filter': FILTER_LANGUAGE,  # ← Nueva información de idioma\n",
    "        'max_words': MAX_WORDS,\n",
    "        'max_length': MAX_LENGTH,\n",
    "        'embedding_dim': EMBEDDING_DIM,\n",
    "        'hidden_layers': HIDDEN_LAYERS,\n",
    "        'activation': ACTIVATION,\n",
    "        'output_activation': OUTPUT_ACTIVATION,\n",
    "        'dropout_rate': DROPOUT_RATE,\n",
    "        'epochs': EPOCHS,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'patience': PATIENCE,\n",
    "        'optimizer': OPTIMIZER,\n",
    "        'loss_function': LOSS_FUNCTION,\n",
    "        'random_seed': RANDOM_SEED,\n",
    "        'total_parameters': total_params,\n",
    "        'gpu_used': gpu_info['gpu_available']\n",
    "    },\n",
    "    'dataset_info': {\n",
    "        'train_samples': len(train_df),\n",
    "        'val_samples': len(val_df),\n",
    "        'test_samples': len(test_df),\n",
    "        'num_classes': num_classes,\n",
    "        'vocab_size': vocab_size,\n",
    "        'class_distribution': class_distribution.to_dict(),\n",
    "        'language_used': FILTER_LANGUAGE if FILTER_LANGUAGE else \"multilingual\"  # ← Nueva información\n",
    "    },\n",
    "    'training_results': training_results,\n",
    "    'evaluation_metrics': {\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'test_loss': test_loss,\n",
    "        'f1_macro': classification_rep['macro avg']['f1-score'],\n",
    "        'f1_weighted': classification_rep['weighted avg']['f1-score'],\n",
    "        'precision_macro': classification_rep['macro avg']['precision'],\n",
    "        'recall_macro': classification_rep['macro avg']['recall'],\n",
    "        'classification_report': classification_rep\n",
    "    },\n",
    "    'gpu_info': gpu_info\n",
    "}\n",
    "\n",
    "# Guardar resultados del experimento\n",
    "results_manager = ResultsManager(output_dir=OUTPUT_DIR)\n",
    "experiment_id = results_manager.save_experiment_results(experiment_data)\n",
    "\n",
    "print(f\"\\nExperimento #{experiment_id} guardado exitosamente.\")\n",
    "print(f\"Idioma utilizado: {FILTER_LANGUAGE if FILTER_LANGUAGE else 'Multiidioma'}\")\n",
    "print(f\"Modelo guardado en: {training_results['model_path']}\")\n",
    "print(f\"Resultados guardados en: {OUTPUT_DIR}/experiment_history.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cc54a1",
   "metadata": {},
   "source": [
    "## 10. Resumen del Experimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b353d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar resumen del experimento actual\n",
    "print(f\"RESUMEN DEL EXPERIMENTO #{experiment_id}\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Modelo: {MODEL_TYPE}\")\n",
    "print(f\"Idioma: {FILTER_LANGUAGE if FILTER_LANGUAGE else 'Multiidioma (todos)'}\")\n",
    "print(f\"Arquitectura: {HIDDEN_LAYERS}\")\n",
    "print(f\"Parámetros totales: {total_params:,}\")\n",
    "print(f\"\")\n",
    "print(f\"Dataset:\")\n",
    "print(f\"  - Entrenamiento: {len(train_df):,} muestras\")\n",
    "print(f\"  - Validación: {len(val_df):,} muestras\")\n",
    "print(f\"  - Prueba: {len(test_df):,} muestras\")\n",
    "print(f\"\")\n",
    "print(f\"Entrenamiento:\")\n",
    "print(f\"  - Épocas: {training_results['epochs_trained']}/{EPOCHS}\")\n",
    "print(f\"  - Tiempo: {training_results['training_time']:.1f}s\")\n",
    "print(f\"  - Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  - Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"\")\n",
    "print(f\"Resultados:\")\n",
    "print(f\"  - Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"  - Test Loss: {test_loss:.4f}\")\n",
    "print(f\"  - F1-Score (macro): {classification_rep['macro avg']['f1-score']:.4f}\")\n",
    "print(f\"  - F1-Score (weighted): {classification_rep['weighted avg']['f1-score']:.4f}\")\n",
    "print(f\"\")\n",
    "print(f\"Hardware: {'GPU' if gpu_info['gpu_available'] else 'CPU'}\")\n",
    "\n",
    "# Mostrar distribución de clases final\n",
    "print(f\"\")\n",
    "print(f\"Distribución de clases utilizadas:\")\n",
    "for stars, count in class_distribution.items():\n",
    "    percentage = (count / len(train_df)) * 100\n",
    "    print(f\"  {stars} estrella(s): {count:,} ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49547771",
   "metadata": {},
   "source": [
    "## 11. Historial de Experimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d1a148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar historial completo de experimentos\n",
    "results_manager.display_experiment_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f358e6",
   "metadata": {},
   "source": [
    "## 12. Análisis de Errores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d97d5c4",
   "metadata": {},
   "source": [
    "## 13. Predicciones de Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f96e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sample_texts(model, tokenizer, sample_texts, class_names, max_length):\n",
    "    \"\"\"\n",
    "    Hacer predicciones en textos de ejemplo.\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo entrenado\n",
    "        tokenizer: Tokenizer usado para entrenar\n",
    "        sample_texts: Lista de textos de ejemplo\n",
    "        class_names: Nombres de las clases\n",
    "        max_length: Longitud máxima de secuencia\n",
    "    \"\"\"\n",
    "    # Procesar textos\n",
    "    sequences = tokenizer.texts_to_sequences(sample_texts)\n",
    "    padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        sequences, maxlen=max_length, padding='post', truncating='post'\n",
    "    )\n",
    "    \n",
    "    # Hacer predicciones\n",
    "    predictions = model.predict(padded)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    print(\"PREDICCIONES DE EJEMPLO:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for i, text in enumerate(sample_texts):\n",
    "        pred_class = predicted_classes[i]\n",
    "        confidence = predictions[i][pred_class]\n",
    "        \n",
    "        print(f\"Texto: {text[:100]}...\")\n",
    "        print(f\"Predicción: {class_names[pred_class]} estrellas (confianza: {confidence:.3f})\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "# Ejemplos de textos para probar - MEJORADOS CON TÍTULOS\n",
    "if USE_TITLE_AND_BODY:\n",
    "    sample_texts = [\n",
    "        \"Excelente producto Este producto es excelente, muy buena calidad y llegó rápido\",\n",
    "        \"Producto terrible Terrible producto, no funciona como se describe\", \n",
    "        \"Producto normal El producto está bien, cumple con lo básico\",\n",
    "        \"Producto increíble Increíble calidad, superó mis expectativas completamente\",\n",
    "        \"No recomendado No recomiendo este producto, muy mala experiencia\"\n",
    "    ]\n",
    "    print(\"Usando formato: [TÍTULO] + [CUERPO] para mejores predicciones\")\n",
    "else:\n",
    "    sample_texts = [\n",
    "        \"Este producto es excelente, muy buena calidad y llegó rápido\",\n",
    "        \"Terrible producto, no funciona como se describe\", \n",
    "        \"El producto está bien, cumple con lo básico\",\n",
    "        \"Increíble calidad, superó mis expectativas completamente\",\n",
    "        \"No recomiendo este producto, muy mala experiencia\"\n",
    "    ]\n",
    "    print(\"Usando formato: solo [CUERPO]\")\n",
    "\n",
    "# Hacer predicciones en ejemplos\n",
    "predict_sample_texts(\n",
    "    model=model,\n",
    "    tokenizer=data_loader.tokenizer,\n",
    "    sample_texts=sample_texts,\n",
    "    class_names=class_names,\n",
    "    max_length=MAX_LENGTH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9278c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analizar algunos errores del modelo\n",
    "def analyze_errors(X_test, y_test, y_pred, test_df, text_column, target_column, n_examples=5):\n",
    "    \"\"\"\n",
    "    Analizar ejemplos donde el modelo se equivocó.\n",
    "    \"\"\"\n",
    "    # Encontrar índices donde el modelo se equivocó\n",
    "    y_test_labels = np.argmax(y_test, axis=1)\n",
    "    error_indices = np.where(y_test_labels != y_pred)[0]\n",
    "    \n",
    "    if len(error_indices) == 0:\n",
    "        print(\"Perfecto. El modelo no cometió errores en el conjunto de prueba.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"ANÁLISIS DE ERRORES ({len(error_indices)} errores total)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Mostrar algunos ejemplos de errores\n",
    "    sample_errors = np.random.choice(error_indices, \n",
    "                                   min(n_examples, len(error_indices)), \n",
    "                                   replace=False)\n",
    "    \n",
    "    for i, idx in enumerate(sample_errors):\n",
    "        true_label = y_test_labels[idx]\n",
    "        pred_label = y_pred[idx]\n",
    "        text = test_df.iloc[idx][text_column]\n",
    "        \n",
    "        print(f\"Error #{i+1}:\")\n",
    "        print(f\"Texto: {text[:150]}...\")\n",
    "        print(f\"Etiqueta real: {true_label + 1} estrellas\")\n",
    "        print(f\"Predicción: {pred_label + 1} estrellas\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "# Realizar análisis de errores\n",
    "analyze_errors(X_test, y_test, y_pred, test_df, TEXT_COLUMN, TARGET_COLUMN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310ia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
