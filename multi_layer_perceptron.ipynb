{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "493885d5",
   "metadata": {},
   "source": [
    "# Proyecto Redes Neuronales: Perceptr√≥n Multicapa\n",
    "\n",
    "**Curso:** Redes Neuronales 2025-II  \n",
    "**Objetivo:** Entrenar y evaluar un modelo de Perceptr√≥n Multicapa (MLP) para clasificaci√≥n de texto seg√∫n el enunciado del proyecto.  \n",
    "**Autor:** Herney Eduardo Quintero Trochez  \n",
    "**Fecha:** 2025  \n",
    "**Universidad:** Universidad Del Valle  \n",
    "\n",
    "## Componentes implementados:\n",
    "1. Configuraci√≥n de Par√°metros Globales\n",
    "2. Carga y Preprocesamiento de Datos\n",
    "3. Tokenizaci√≥n y Creaci√≥n del Vocabulario\n",
    "4. Construcci√≥n del Modelo MLP\n",
    "5. Entrenamiento con Early Stopping\n",
    "6. Evaluaci√≥n del Modelo\n",
    "7. Guardado de Resultados y Modelos\n",
    "8. Visualizaci√≥n de Resultados\n",
    "9. Historial de Experimentos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f37abf",
   "metadata": {},
   "source": [
    "## 0. Configuraci√≥n de Par√°metros Globales\n",
    "\n",
    "Esta secci√≥n permite modificar f√°cilmente todos los par√°metros del modelo para experimentar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b42a10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CONFIGURACI√ìN DE PAR√ÅMETROS GLOBALES =====\n",
    "# Esta secci√≥n centraliza todos los par√°metros del modelo para facilitar experimentaci√≥n\n",
    "\n",
    "# Configuraci√≥n general del experimento\n",
    "EXPERIMENT_NAME = \"MultiLayer_Perceptron\"\n",
    "MODEL_TYPE = \"MLP\"  # Perceptr√≥n Multi-Capa  \n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Directorios de trabajo\n",
    "DATA_DIR = \"data\"\n",
    "MODEL_DIR = \"models\"\n",
    "OUTPUT_DIR = \"output\"\n",
    "\n",
    "# Configuraci√≥n del dataset - MEJORADO CON T√çTULO\n",
    "TEXT_COLUMN = \"review_body\"  # Columna con el texto del cuerpo de la rese√±a\n",
    "TITLE_COLUMN = \"review_title\"  # Columna con el t√≠tulo de la rese√±a\n",
    "TARGET_COLUMN = \"stars\"  # Columna con las etiquetas (1-5 estrellas)\n",
    "LANGUAGE_COLUMN = \"language\"  # Columna con el idioma\n",
    "FILTER_LANGUAGE = \"en\"  # Filtrar por idioma espec√≠fico. Opciones: None, \"en\", \"es\", \"de\", \"fr\", \"ja\"\n",
    "USE_TITLE_AND_BODY = True  # NUEVO: Usar tanto t√≠tulo como cuerpo para mejor precisi√≥n\n",
    "MAX_WORDS = 80000  # Tama√±o m√°ximo del vocabulario\n",
    "MAX_LENGTH = 300  # Longitud m√°xima de las secuencias (aumentada por combinar t√≠tulo+cuerpo)\n",
    "OOV_TOKEN = \"<OOV>\"  # Token para palabras fuera del vocabulario\n",
    "\n",
    "# Par√°metros de arquitectura del modelo - MLP\n",
    "EMBEDDING_DIM = 300  # Dimensi√≥n del embedding de palabras\n",
    "HIDDEN_LAYERS = [256, 128, 64]  # Lista con el n√∫mero de neuronas en cada capa oculta\n",
    "ACTIVATION = \"relu\"  # Funci√≥n de activaci√≥n para capas ocultas\n",
    "OUTPUT_ACTIVATION = \"softmax\"  # Funci√≥n de activaci√≥n para la capa de salida\n",
    "DROPOUT_RATE = 0.3  # Tasa de dropout para regularizaci√≥n\n",
    "\n",
    "# Par√°metros de entrenamiento\n",
    "EPOCHS = 50  # N√∫mero m√°ximo de √©pocas de entrenamiento\n",
    "BATCH_SIZE = 512  # Tama√±o del batch\n",
    "LEARNING_RATE = 0.001  # Tasa de aprendizaje\n",
    "PATIENCE = 10  # Paciencia para early stopping\n",
    "OPTIMIZER = \"adam\"  # Optimizador a usar\n",
    "LOSS_FUNCTION = \"categorical_crossentropy\"  # Funci√≥n de p√©rdida\n",
    "METRICS = [\"accuracy\"]  # M√©tricas a monitorear\n",
    "\n",
    "print(f\"=== Configuraci√≥n del Experimento: {EXPERIMENT_NAME} ===\")\n",
    "print(f\"Modelo: {MODEL_TYPE}\")\n",
    "print(f\"Filtro de idioma: {FILTER_LANGUAGE if FILTER_LANGUAGE else 'Multiidioma'}\")\n",
    "print(f\"Usar t√≠tulo + cuerpo: {USE_TITLE_AND_BODY}\")\n",
    "print(f\"Longitud m√°xima: {MAX_LENGTH} tokens\")\n",
    "print(f\"Arquitectura oculta: {HIDDEN_LAYERS}\")\n",
    "print(f\"Dimensi√≥n embedding: {EMBEDDING_DIM}\")\n",
    "print(f\"Dropout: {DROPOUT_RATE}\")\n",
    "print(f\"√âpocas m√°ximas: {EPOCHS}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Tasa de aprendizaje: {LEARNING_RATE}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6ffb44",
   "metadata": {},
   "source": [
    "## 1. Importaci√≥n de Librer√≠as y Funciones Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f08898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librer√≠as est√°ndar\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Librer√≠as de machine learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Importar funciones helper\n",
    "from helper import (\n",
    "    DataLoader, ModelTrainer, ResultsManager, Visualizer,\n",
    "    evaluate_model, get_gpu_info, setup_experiment_environment\n",
    ")\n",
    "\n",
    "# Configurar ambiente del experimento\n",
    "gpu_info = setup_experiment_environment(RANDOM_SEED)\n",
    "print(f\"Ambiente configurado. GPU disponible: {gpu_info['gpu_available']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbf34a3",
   "metadata": {},
   "source": [
    "## 2. Carga y Preprocesamiento de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e63e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar el cargador de datos\n",
    "data_loader = DataLoader(data_dir=DATA_DIR)\n",
    "\n",
    "# Cargar los datasets\n",
    "print(\"Cargando datasets...\")\n",
    "train_df, val_df, test_df = data_loader.load_all_data()\n",
    "\n",
    "print(f\"\\nDatos originales cargados:\")\n",
    "print(f\"Entrenamiento: {len(train_df)} muestras\")\n",
    "print(f\"Validaci√≥n: {len(val_df)} muestras\") \n",
    "print(f\"Prueba: {len(test_df)} muestras\")\n",
    "\n",
    "# Verificar que las columnas necesarias existen\n",
    "required_columns = [TEXT_COLUMN, TARGET_COLUMN, LANGUAGE_COLUMN]\n",
    "if USE_TITLE_AND_BODY:\n",
    "    required_columns.append(TITLE_COLUMN)\n",
    "\n",
    "missing_columns = [col for col in required_columns if col not in train_df.columns]\n",
    "if missing_columns:\n",
    "    print(f\"Columnas faltantes: {missing_columns}\")\n",
    "    print(f\"Columnas disponibles: {list(train_df.columns)}\")\n",
    "else:\n",
    "    print(f\"Todas las columnas requeridas est√°n disponibles\")\n",
    "    if USE_TITLE_AND_BODY:\n",
    "        print(f\"Modo combinado: {TITLE_COLUMN} + {TEXT_COLUMN}\")\n",
    "\n",
    "# Analizar distribuci√≥n de idiomas\n",
    "print(f\"\\nAn√°lisis de idiomas en el dataset:\")\n",
    "if LANGUAGE_COLUMN in train_df.columns:\n",
    "    lang_dist_train = train_df[LANGUAGE_COLUMN].value_counts()\n",
    "    print(f\"Distribuci√≥n de idiomas (entrenamiento):\")\n",
    "    for lang, count in lang_dist_train.items():\n",
    "        percentage = (count / len(train_df)) * 100\n",
    "        print(f\"  {lang}: {count:,} muestras ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Aplicar filtro por idioma si est√° especificado\n",
    "    if FILTER_LANGUAGE is not None:\n",
    "        if FILTER_LANGUAGE in lang_dist_train.index:\n",
    "            print(f\"\\nFiltrando por idioma: {FILTER_LANGUAGE}\")\n",
    "            \n",
    "            # Filtrar datasets por idioma\n",
    "            train_df = train_df[train_df[LANGUAGE_COLUMN] == FILTER_LANGUAGE].copy()\n",
    "            val_df = val_df[val_df[LANGUAGE_COLUMN] == FILTER_LANGUAGE].copy()\n",
    "            test_df = test_df[test_df[LANGUAGE_COLUMN] == FILTER_LANGUAGE].copy()\n",
    "            \n",
    "            print(f\"\\nDatos despu√©s del filtrado por idioma '{FILTER_LANGUAGE}':\")\n",
    "            print(f\"Entrenamiento: {len(train_df)} muestras\")\n",
    "            print(f\"Validaci√≥n: {len(val_df)} muestras\")\n",
    "            print(f\"Prueba: {len(test_df)} muestras\")\n",
    "        else:\n",
    "            print(f\"\\nAdvertencia: Idioma '{FILTER_LANGUAGE}' no encontrado en el dataset.\")\n",
    "            print(f\"Idiomas disponibles: {list(lang_dist_train.index)}\")\n",
    "            print(\"Usando todos los idiomas...\")\n",
    "    else:\n",
    "        print(f\"\\nUsando todos los idiomas disponibles\")\n",
    "else:\n",
    "    print(f\"Columna '{LANGUAGE_COLUMN}' no encontrada. Usando todos los datos sin filtrar.\")\n",
    "\n",
    "# Mostrar distribuci√≥n de clases en el conjunto final\n",
    "print(f\"\\nDistribuci√≥n de clases (conjunto final):\")\n",
    "class_distribution = train_df[TARGET_COLUMN].value_counts().sort_index()\n",
    "for stars, count in class_distribution.items():\n",
    "    percentage = (count / len(train_df)) * 100\n",
    "    print(f\"  {stars} estrella(s): {count:,} muestras ({percentage:.1f}%)\")\n",
    "\n",
    "# Mostrar ejemplos de datos con t√≠tulo y cuerpo\n",
    "print(f\"\\nEjemplos de datos del conjunto final:\")\n",
    "for i in range(min(3, len(train_df))):\n",
    "    lang = train_df[LANGUAGE_COLUMN].iloc[i] if LANGUAGE_COLUMN in train_df.columns else \"N/A\"\n",
    "    title = train_df[TITLE_COLUMN].iloc[i] if USE_TITLE_AND_BODY and TITLE_COLUMN in train_df.columns else \"N/A\"\n",
    "    text = train_df[TEXT_COLUMN].iloc[i][:80]  # Menos texto para mostrar t√≠tulo tambi√©n\n",
    "    stars = train_df[TARGET_COLUMN].iloc[i]\n",
    "    \n",
    "    print(f\"{i+1}. [{lang}] {stars} estrella(s)\")\n",
    "    if USE_TITLE_AND_BODY and title != \"N/A\":\n",
    "        print(f\"   T√≠tulo: {title[:60]}{'...' if len(str(title)) > 60 else ''}\")\n",
    "    print(f\"   Cuerpo: {text}{'...' if len(str(train_df[TEXT_COLUMN].iloc[i])) > 80 else ''}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5073b8ca",
   "metadata": {},
   "source": [
    "### Configuraciones R√°pidas por Idioma\n",
    "\n",
    "Para cambiar el idioma del experimento, modifica la variable `FILTER_LANGUAGE` en la primera celda:\n",
    "\n",
    "```python\n",
    "# Opciones disponibles:\n",
    "FILTER_LANGUAGE = None     # Todos los idiomas (por defecto)\n",
    "FILTER_LANGUAGE = \"en\"     # Solo ingl√©s \n",
    "FILTER_LANGUAGE = \"es\"     # Solo espa√±ol\n",
    "FILTER_LANGUAGE = \"de\"     # Solo alem√°n\n",
    "FILTER_LANGUAGE = \"fr\"     # Solo franc√©s\n",
    "FILTER_LANGUAGE = \"ja\"     # Solo japon√©s\n",
    "```\n",
    "\n",
    "**Recomendaciones:**\n",
    "- **`\"en\"` (Ingl√©s)**: Mayor cantidad de datos, vocabulario m√°s rico\n",
    "- **`\"es\"` (Espa√±ol)**: Bueno para comparar rendimiento en espa√±ol\n",
    "- **`None` (Todos)**: Para experimentos multiidioma\n",
    "- **Otros idiomas**: Para an√°lisis espec√≠ficos por idioma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da98c311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURACI√ìN R√ÅPIDA - Descomenta una l√≠nea para cambiar idioma\n",
    "# FILTER_LANGUAGE = None      # Por defecto: todos los idiomas\n",
    "# FILTER_LANGUAGE = \"en\"      # Solo ingl√©s (recomendado para mejor rendimiento)\n",
    "# FILTER_LANGUAGE = \"es\"      # Solo espa√±ol \n",
    "# FILTER_LANGUAGE = \"de\"      # Solo alem√°n\n",
    "# FILTER_LANGUAGE = \"fr\"      # Solo franc√©s\n",
    "# FILTER_LANGUAGE = \"ja\"      # Solo japon√©s\n",
    "\n",
    "# Si cambias el idioma aqu√≠, ejecuta esta celda y vuelve a ejecutar desde la carga de datos\n",
    "print(f\"Configuraci√≥n actual: {FILTER_LANGUAGE if FILTER_LANGUAGE else 'Todos los idiomas'}\")\n",
    "print(\"Tip: Para ingl√©s √∫nicamente (mejor rendimiento), descomenta: FILTER_LANGUAGE = 'en'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26214504",
   "metadata": {},
   "source": [
    "## 3. Preprocesamiento de Texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec025b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesar los datos de texto\n",
    "print(\"Preprocesando datos de texto...\")\n",
    "print(f\"Modo: {'T√≠tulo + Cuerpo' if USE_TITLE_AND_BODY else 'Solo Cuerpo'}\")\n",
    "\n",
    "processed_data = data_loader.preprocess_text_data_embedding(\n",
    "    train_df=train_df,\n",
    "    val_df=val_df, \n",
    "    test_df=test_df,\n",
    "    text_column=TEXT_COLUMN,\n",
    "    title_column=TITLE_COLUMN if USE_TITLE_AND_BODY else None,\n",
    "    target_column=TARGET_COLUMN,\n",
    "    max_words=MAX_WORDS,\n",
    "    max_length=MAX_LENGTH,\n",
    "    use_title_and_body=USE_TITLE_AND_BODY\n",
    ")\n",
    "\n",
    "# Extraer datos preprocesados\n",
    "X_train, y_train = processed_data['X_train'], processed_data['y_train']\n",
    "X_val, y_val = processed_data['X_val'], processed_data['y_val']\n",
    "X_test, y_test = processed_data['X_test'], processed_data['y_test']\n",
    "num_classes = processed_data['num_classes']\n",
    "vocab_size = processed_data['vocab_size']\n",
    "\n",
    "print(f\"\\nDatos preprocesados:\")\n",
    "print(f\"Tama√±o del vocabulario: {vocab_size}\")\n",
    "print(f\"N√∫mero de clases: {num_classes}\")\n",
    "print(f\"Forma de X_train: {X_train.shape}\")\n",
    "print(f\"Forma de y_train: {y_train.shape}\")\n",
    "print(f\"Texto combinado: {'S√≠ (t√≠tulo + cuerpo)' if USE_TITLE_AND_BODY else 'No (solo cuerpo)'}\")\n",
    "\n",
    "# Obtener nombres de clases para evaluaci√≥n\n",
    "class_names = [str(i) for i in data_loader.label_encoder.classes_]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d2f4be",
   "metadata": {},
   "source": [
    "### Mejora Implementada: T√≠tulo + Cuerpo\n",
    "\n",
    "**Ventajas de combinar t√≠tulo y cuerpo:**\n",
    "- **M√°s contexto:** El t√≠tulo often contiene informaci√≥n clave sobre el sentiment\n",
    "- **Mejor precisi√≥n:** M√°s informaci√≥n textual para el modelo\n",
    "- **Vocabulario enriquecido:** Palabras clave del t√≠tulo complementan el cuerpo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94952bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos algunos ejemplos de c√≥mo se ve el texto combinado\n",
    "print(\"EJEMPLOS DE TEXTO COMBINADO (T√≠tulo + Cuerpo):\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Seleccionar algunos ejemplos del conjunto de entrenamiento\n",
    "for i in range(3):\n",
    "    title = train_df.iloc[i]['review_title']\n",
    "    body = train_df.iloc[i]['review_body']\n",
    "    combined = f\"{title} {body}\"\n",
    "    stars = train_df.iloc[i]['stars']\n",
    "    \n",
    "    print(f\"\\nEjemplo {i+1} - {stars} estrella(s):\")\n",
    "    print(f\"T√≠tulo: {title}\")\n",
    "    print(f\"Cuerpo: {body[:100]}...\")\n",
    "    print(f\"Combinado: {combined[:150]}...\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "print(f\"\\nBeneficios de la combinaci√≥n:\")\n",
    "print(f\"   Mayor contexto sem√°ntico\")\n",
    "print(f\"   Informaci√≥n clave del t√≠tulo se preserva\")  \n",
    "print(f\"   Vocabulario m√°s rico para el modelo\")\n",
    "print(f\"   Longitud m√°xima aumentada a {MAX_LENGTH} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295adb63",
   "metadata": {},
   "source": [
    "### Funci√≥n para Comparar Configuraciones\n",
    "\n",
    "Esta celda permite cambiar f√°cilmente entre usar solo el cuerpo o t√≠tulo+cuerpo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e25a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURACI√ìN R√ÅPIDA - Cambia aqu√≠ para experimentar\n",
    "print(\"CONFIGURACIONES DISPONIBLES:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"1. T√≠tulo + Cuerpo (ACTUAL)\")\n",
    "print(\"   - USE_TITLE_AND_BODY = True\")\n",
    "print(\"   - MAX_LENGTH = 300\")\n",
    "print(\"   - Mejor precisi√≥n esperada\")\n",
    "print()\n",
    "print(\"2. Solo Cuerpo (TRADICIONAL)\") \n",
    "print(\"   - USE_TITLE_AND_BODY = False\")\n",
    "print(\"   - MAX_LENGTH = 250\")\n",
    "print(\"   - M√°s r√°pido de procesar\")\n",
    "print()\n",
    "print(f\"Configuraci√≥n actual: {'T√≠tulo + Cuerpo' if USE_TITLE_AND_BODY else 'Solo Cuerpo'}\")\n",
    "print(f\"Longitud m√°xima: {MAX_LENGTH} tokens\")\n",
    "print(f\"Vocabulario: {vocab_size:,} palabras\")\n",
    "print()\n",
    "print(\"Para cambiar configuraci√≥n:\")\n",
    "print(\"   1. Modifica USE_TITLE_AND_BODY en la primera celda\")\n",
    "print(\"   2. Ajusta MAX_LENGTH seg√∫n necesites\")\n",
    "print(\"   3. Re-ejecuta desde la carga de datos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bow_comparison_header",
   "metadata": {},
   "source": [
    "### Comparaci√≥n: Embedding vs BoW/TF-IDF\n",
    "\n",
    "**Nuevo:** Ahora puedes elegir entre dos enfoques de preprocesamiento:\n",
    "- **Embedding (usado arriba):** Secuencias num√©ricas para capas de embedding\n",
    "- **BoW/TF-IDF:** Matrices dispersas optimizadas en memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bow_comparison_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEMOSTRACI√ìN: Comparaci√≥n de enfoques de preprocesamiento\n",
    "print(\"COMPARACI√ìN DE ENFOQUES DE PREPROCESAMIENTO\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Crear un data loader separado para la comparaci√≥n\n",
    "comparison_loader = DataLoader(data_dir=DATA_DIR)\n",
    "\n",
    "# Usar una muestra peque√±a para demostraci√≥n r√°pida\n",
    "sample_size = min(1000, len(train_df))\n",
    "train_sample = train_df.sample(sample_size).reset_index(drop=True)\n",
    "val_sample = val_df.sample(min(200, len(val_df))).reset_index(drop=True)\n",
    "test_sample = test_df.sample(min(200, len(test_df))).reset_index(drop=True)\n",
    "\n",
    "print(f\"Usando muestra de {sample_size} registros de entrenamiento para comparaci√≥n r√°pida...\\n\")\n",
    "\n",
    "# 1. Enfoque Embedding (ya usado arriba)\n",
    "print(\"1. ENFOQUE EMBEDDING (actual):\")\n",
    "print(\"-\" * 30)\n",
    "embedding_data = comparison_loader.preprocess_text_data_embedding(\n",
    "    train_sample, val_sample, test_sample,\n",
    "    text_column=TEXT_COLUMN,\n",
    "    title_column=TITLE_COLUMN if USE_TITLE_AND_BODY else None,\n",
    "    target_column=TARGET_COLUMN,\n",
    "    max_words=MAX_WORDS,\n",
    "    max_length=MAX_LENGTH,\n",
    "    use_title_and_body=USE_TITLE_AND_BODY\n",
    ")\n",
    "\n",
    "embedding_memory = embedding_data['X_train'].nbytes / (1024 * 1024)\n",
    "print(f\"   Memoria: {embedding_memory:.1f} MB\")\n",
    "print(f\"   Forma: {embedding_data['X_train'].shape}\")\n",
    "print(f\"   Tipo: Matriz densa (numpy array)\")\n",
    "\n",
    "print(\"\\n2. ENFOQUE BOW/TF-IDF (optimizado):\")\n",
    "print(\"-\" * 35)\n",
    "bow_data = comparison_loader.preprocess_text_data_bow(\n",
    "    train_sample, val_sample, test_sample,\n",
    "    text_column=TEXT_COLUMN,\n",
    "    title_column=TITLE_COLUMN if USE_TITLE_AND_BODY else None,\n",
    "    target_column=TARGET_COLUMN,\n",
    "    max_features=5000,  # Optimizado seg√∫n especificaciones\n",
    "    min_df=3,\n",
    "    max_df=0.85,\n",
    "    use_tfidf=True,\n",
    "    use_title_and_body=USE_TITLE_AND_BODY\n",
    ")\n",
    "\n",
    "print(f\"   Memoria: {bow_data['memory_mb']:.1f} MB\")\n",
    "print(f\"   Forma: {bow_data['X_train'].shape}\")\n",
    "print(f\"   Tipo: Matriz dispersa (scipy sparse)\")\n",
    "print(f\"   Sparsity: {bow_data['sparsity']:.1%}\")\n",
    "\n",
    "# Comparaci√≥n final\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RESUMEN DE COMPARACI√ìN:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if embedding_memory > 0:\n",
    "    memory_savings = embedding_memory - bow_data['memory_mb']\n",
    "    savings_pct = (memory_savings / embedding_memory) * 100\n",
    "    print(f\"üíæ Ahorro de memoria: {memory_savings:.1f} MB ({savings_pct:.1f}%)\")\n",
    "else:\n",
    "    print(f\"üíæ BoW usa: {bow_data['memory_mb']:.1f} MB (vs embedding)\")\n",
    "\n",
    "print(f\"üìä Caracter√≠sticas: {bow_data['vocab_size']} (BoW) vs {embedding_data['vocab_size']} (Embedding)\")\n",
    "print(f\"üéØ Sparsity: {bow_data['sparsity']:.1%} (BoW es {bow_data['sparsity']:.1%} vac√≠o)\")\n",
    "print(f\"‚ö° Optimizaciones BoW: vocabulario limitado, matrices dispersas, filtrado de t√©rminos\")\n",
    "\n",
    "print(\"\\nüí° Recomendaci√≥n: Usar BoW para datasets grandes (>100K muestras) por eficiencia de memoria\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a4f8d1",
   "metadata": {},
   "source": [
    "## 4. Construcci√≥n del Modelo MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cf91a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp_model(vocab_size, embedding_dim, max_length, hidden_layers, \n",
    "                     num_classes, dropout_rate, activation, output_activation):\n",
    "    \"\"\"\n",
    "    Crear un modelo de Perceptr√≥n Multicapa para clasificaci√≥n de texto.\n",
    "    \n",
    "    Args:\n",
    "        vocab_size: Tama√±o del vocabulario\n",
    "        embedding_dim: Dimensi√≥n del embedding\n",
    "        max_length: Longitud m√°xima de secuencia\n",
    "        hidden_layers: Lista con el n√∫mero de neuronas en cada capa oculta\n",
    "        num_classes: N√∫mero de clases de salida\n",
    "        dropout_rate: Tasa de dropout\n",
    "        activation: Funci√≥n de activaci√≥n para capas ocultas\n",
    "        output_activation: Funci√≥n de activaci√≥n para la capa de salida\n",
    "    \n",
    "    Returns:\n",
    "        tf.keras.Model: Modelo compilado\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Capa de embedding (sin input_length que est√° deprecado)\n",
    "    model.add(Embedding(input_dim=vocab_size, \n",
    "                       output_dim=embedding_dim,\n",
    "                       name=\"embedding_layer\"))\n",
    "    \n",
    "    # Pooling global para reducir dimensionalidad\n",
    "    model.add(GlobalAveragePooling1D(name=\"global_avg_pooling\"))\n",
    "    \n",
    "    # Capas ocultas del MLP\n",
    "    for i, units in enumerate(hidden_layers):\n",
    "        model.add(Dense(units=units, \n",
    "                       activation=activation, \n",
    "                       name=f\"dense_layer_{i+1}\"))\n",
    "        model.add(Dropout(rate=dropout_rate, \n",
    "                         name=f\"dropout_{i+1}\"))\n",
    "    \n",
    "    # Capa de salida\n",
    "    model.add(Dense(units=num_classes, \n",
    "                   activation=output_activation, \n",
    "                   name=\"output_layer\"))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Crear el modelo\n",
    "print(\"Creando modelo MLP...\")\n",
    "model = create_mlp_model(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    max_length=MAX_LENGTH,\n",
    "    hidden_layers=HIDDEN_LAYERS,\n",
    "    num_classes=num_classes,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    activation=ACTIVATION,\n",
    "    output_activation=OUTPUT_ACTIVATION\n",
    ")\n",
    "\n",
    "# Compilar el modelo\n",
    "optimizer = Adam(learning_rate=LEARNING_RATE)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=LOSS_FUNCTION,\n",
    "    metrics=METRICS\n",
    ")\n",
    "\n",
    "# Construir el modelo con la forma de entrada espec√≠fica\n",
    "model.build(input_shape=(None, MAX_LENGTH))\n",
    "\n",
    "# Mostrar arquitectura del modelo\n",
    "print(\"\\nArquitectura del modelo:\")\n",
    "model.summary()\n",
    "\n",
    "# Contar par√°metros\n",
    "total_params = model.count_params()\n",
    "print(f\"\\nTotal de par√°metros: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e33caa8",
   "metadata": {},
   "source": [
    "## 5. Entrenamiento del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071b8782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar el entrenador de modelos\n",
    "model_trainer = ModelTrainer(model_dir=MODEL_DIR)\n",
    "\n",
    "# Entrenar el modelo\n",
    "print(\"Iniciando entrenamiento...\")\n",
    "training_results = model_trainer.train_model(\n",
    "    model=model,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    patience=PATIENCE,\n",
    "    model_name=f\"{MODEL_TYPE}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    ")\n",
    "\n",
    "print(f\"\\nEntrenamiento completado:\")\n",
    "print(f\"√âpocas entrenadas: {training_results['epochs_trained']}\")\n",
    "print(f\"Tiempo de entrenamiento: {training_results['training_time']:.1f} segundos\")\n",
    "print(f\"Accuracy final (entrenamiento): {training_results['final_train_accuracy']:.4f}\")\n",
    "print(f\"Accuracy final (validaci√≥n): {training_results['final_val_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24905dfc",
   "metadata": {},
   "source": [
    "## 6. Visualizaci√≥n del Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c99001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar el historial de entrenamiento\n",
    "Visualizer.plot_training_history(\n",
    "    history=training_results['history'],\n",
    "    model_name=MODEL_TYPE,\n",
    "    save_path=os.path.join(OUTPUT_DIR, f\"{MODEL_TYPE}_training_history.png\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4484364",
   "metadata": {},
   "source": [
    "## 7. Evaluaci√≥n del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3f6730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar el modelo en el conjunto de prueba\n",
    "evaluation_results = evaluate_model(\n",
    "    model=model,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    class_names=class_names\n",
    ")\n",
    "\n",
    "# Extraer m√©tricas de evaluaci√≥n\n",
    "test_accuracy = evaluation_results['test_accuracy']\n",
    "test_loss = evaluation_results['test_loss']\n",
    "classification_rep = evaluation_results['classification_report']\n",
    "y_true = evaluation_results['y_true']\n",
    "y_pred = evaluation_results['y_pred']\n",
    "\n",
    "print(f\"\\nResultados en conjunto de prueba:\")\n",
    "print(f\"Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Loss: {test_loss:.4f}\")\n",
    "print(f\"F1-Score (macro): {classification_rep['macro avg']['f1-score']:.4f}\")\n",
    "print(f\"F1-Score (weighted): {classification_rep['weighted avg']['f1-score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc34bb6f",
   "metadata": {},
   "source": [
    "## 8. Matriz de Confusi√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4183361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar matriz de confusi√≥n\n",
    "Visualizer.plot_confusion_matrix(\n",
    "    y_true=y_true,\n",
    "    y_pred=y_pred,\n",
    "    class_names=class_names,\n",
    "    model_name=MODEL_TYPE,\n",
    "    save_path=os.path.join(OUTPUT_DIR, f\"{MODEL_TYPE}_confusion_matrix.png\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bae31f",
   "metadata": {},
   "source": [
    "## 9. Guardado de Resultados del Experimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625c28f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar datos del experimento para guardar\n",
    "experiment_data = {\n",
    "    'experiment_name': EXPERIMENT_NAME,\n",
    "    'configuration': {\n",
    "        'model_type': MODEL_TYPE,\n",
    "        'text_column': TEXT_COLUMN,\n",
    "        'target_column': TARGET_COLUMN,\n",
    "        'language_filter': FILTER_LANGUAGE,  # ‚Üê Nueva informaci√≥n de idioma\n",
    "        'max_words': MAX_WORDS,\n",
    "        'max_length': MAX_LENGTH,\n",
    "        'embedding_dim': EMBEDDING_DIM,\n",
    "        'hidden_layers': HIDDEN_LAYERS,\n",
    "        'activation': ACTIVATION,\n",
    "        'output_activation': OUTPUT_ACTIVATION,\n",
    "        'dropout_rate': DROPOUT_RATE,\n",
    "        'epochs': EPOCHS,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'patience': PATIENCE,\n",
    "        'optimizer': OPTIMIZER,\n",
    "        'loss_function': LOSS_FUNCTION,\n",
    "        'random_seed': RANDOM_SEED,\n",
    "        'total_parameters': total_params,\n",
    "        'gpu_used': gpu_info['gpu_available']\n",
    "    },\n",
    "    'dataset_info': {\n",
    "        'train_samples': len(train_df),\n",
    "        'val_samples': len(val_df),\n",
    "        'test_samples': len(test_df),\n",
    "        'num_classes': num_classes,\n",
    "        'vocab_size': vocab_size,\n",
    "        'class_distribution': class_distribution.to_dict(),\n",
    "        'language_used': FILTER_LANGUAGE if FILTER_LANGUAGE else \"multilingual\"  # ‚Üê Nueva informaci√≥n\n",
    "    },\n",
    "    'training_results': training_results,\n",
    "    'evaluation_metrics': {\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'test_loss': test_loss,\n",
    "        'f1_macro': classification_rep['macro avg']['f1-score'],\n",
    "        'f1_weighted': classification_rep['weighted avg']['f1-score'],\n",
    "        'precision_macro': classification_rep['macro avg']['precision'],\n",
    "        'recall_macro': classification_rep['macro avg']['recall'],\n",
    "        'classification_report': classification_rep\n",
    "    },\n",
    "    'gpu_info': gpu_info\n",
    "}\n",
    "\n",
    "# Guardar resultados del experimento\n",
    "results_manager = ResultsManager(output_dir=OUTPUT_DIR)\n",
    "experiment_id = results_manager.save_experiment_results(experiment_data)\n",
    "\n",
    "print(f\"\\nExperimento #{experiment_id} guardado exitosamente.\")\n",
    "print(f\"Idioma utilizado: {FILTER_LANGUAGE if FILTER_LANGUAGE else 'Multiidioma'}\")\n",
    "print(f\"Modelo guardado en: {training_results['model_path']}\")\n",
    "print(f\"Resultados guardados en: {OUTPUT_DIR}/experiment_history.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cc54a1",
   "metadata": {},
   "source": [
    "## 10. Resumen del Experimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b353d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar resumen del experimento actual\n",
    "print(f\"RESUMEN DEL EXPERIMENTO #{experiment_id}\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Modelo: {MODEL_TYPE}\")\n",
    "print(f\"Idioma: {FILTER_LANGUAGE if FILTER_LANGUAGE else 'Multiidioma (todos)'}\")\n",
    "print(f\"Arquitectura: {HIDDEN_LAYERS}\")\n",
    "print(f\"Par√°metros totales: {total_params:,}\")\n",
    "print(f\"\")\n",
    "print(f\"Dataset:\")\n",
    "print(f\"  - Entrenamiento: {len(train_df):,} muestras\")\n",
    "print(f\"  - Validaci√≥n: {len(val_df):,} muestras\")\n",
    "print(f\"  - Prueba: {len(test_df):,} muestras\")\n",
    "print(f\"\")\n",
    "print(f\"Entrenamiento:\")\n",
    "print(f\"  - √âpocas: {training_results['epochs_trained']}/{EPOCHS}\")\n",
    "print(f\"  - Tiempo: {training_results['training_time']:.1f}s\")\n",
    "print(f\"  - Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  - Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"\")\n",
    "print(f\"Resultados:\")\n",
    "print(f\"  - Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"  - Test Loss: {test_loss:.4f}\")\n",
    "print(f\"  - F1-Score (macro): {classification_rep['macro avg']['f1-score']:.4f}\")\n",
    "print(f\"  - F1-Score (weighted): {classification_rep['weighted avg']['f1-score']:.4f}\")\n",
    "print(f\"\")\n",
    "print(f\"Hardware: {'GPU' if gpu_info['gpu_available'] else 'CPU'}\")\n",
    "\n",
    "# Mostrar distribuci√≥n de clases final\n",
    "print(f\"\")\n",
    "print(f\"Distribuci√≥n de clases utilizadas:\")\n",
    "for stars, count in class_distribution.items():\n",
    "    percentage = (count / len(train_df)) * 100\n",
    "    print(f\"  {stars} estrella(s): {count:,} ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49547771",
   "metadata": {},
   "source": [
    "## 11. Historial de Experimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d1a148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar historial completo de experimentos\n",
    "results_manager.display_experiment_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d97d5c4",
   "metadata": {},
   "source": [
    "## 12. Predicciones de Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f96e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sample_texts(model, tokenizer, sample_texts, class_names, max_length):\n",
    "    \"\"\"\n",
    "    Hacer predicciones en textos de ejemplo.\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo entrenado\n",
    "        tokenizer: Tokenizer usado para entrenar\n",
    "        sample_texts: Lista de textos de ejemplo\n",
    "        class_names: Nombres de las clases\n",
    "        max_length: Longitud m√°xima de secuencia\n",
    "    \"\"\"\n",
    "    # Procesar textos\n",
    "    sequences = tokenizer.texts_to_sequences(sample_texts)\n",
    "    padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        sequences, maxlen=max_length, padding='post', truncating='post'\n",
    "    )\n",
    "    \n",
    "    # Hacer predicciones\n",
    "    predictions = model.predict(padded)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    print(\"PREDICCIONES DE EJEMPLO:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for i, text in enumerate(sample_texts):\n",
    "        pred_class = predicted_classes[i]\n",
    "        confidence = predictions[i][pred_class]\n",
    "        \n",
    "        print(f\"Texto: {text[:100]}...\")\n",
    "        print(f\"Predicci√≥n: {class_names[pred_class]} estrellas (confianza: {confidence:.3f})\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "# Ejemplos de textos para probar - MEJORADOS CON T√çTULOS\n",
    "if USE_TITLE_AND_BODY:\n",
    "    sample_texts = [\n",
    "        \"Excelente producto Este producto es excelente, muy buena calidad y lleg√≥ r√°pido\",\n",
    "        \"Producto terrible Terrible producto, no funciona como se describe\", \n",
    "        \"Producto normal El producto est√° bien, cumple con lo b√°sico\",\n",
    "        \"Producto incre√≠ble Incre√≠ble calidad, super√≥ mis expectativas completamente\",\n",
    "        \"No recomendado No recomiendo este producto, muy mala experiencia\"\n",
    "    ]\n",
    "    print(\"Usando formato: [T√çTULO] + [CUERPO] para mejores predicciones\")\n",
    "else:\n",
    "    sample_texts = [\n",
    "        \"Este producto es excelente, muy buena calidad y lleg√≥ r√°pido\",\n",
    "        \"Terrible producto, no funciona como se describe\", \n",
    "        \"El producto est√° bien, cumple con lo b√°sico\",\n",
    "        \"Incre√≠ble calidad, super√≥ mis expectativas completamente\",\n",
    "        \"No recomiendo este producto, muy mala experiencia\"\n",
    "    ]\n",
    "    print(\"Usando formato: solo [CUERPO]\")\n",
    "\n",
    "# Hacer predicciones en ejemplos\n",
    "predict_sample_texts(\n",
    "    model=model,\n",
    "    tokenizer=data_loader.tokenizer,\n",
    "    sample_texts=sample_texts,\n",
    "    class_names=class_names,\n",
    "    max_length=MAX_LENGTH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f358e6",
   "metadata": {},
   "source": [
    "## 13. An√°lisis de Errores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9278c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analizar algunos errores del modelo\n",
    "def analyze_errors(X_test, y_test, y_pred, test_df, text_column, target_column, n_examples=5):\n",
    "    \"\"\"\n",
    "    Analizar ejemplos donde el modelo se equivoc√≥.\n",
    "    \"\"\"\n",
    "    # Encontrar √≠ndices donde el modelo se equivoc√≥\n",
    "    y_test_labels = np.argmax(y_test, axis=1)\n",
    "    error_indices = np.where(y_test_labels != y_pred)[0]\n",
    "    \n",
    "    if len(error_indices) == 0:\n",
    "        print(\"Perfecto. El modelo no cometi√≥ errores en el conjunto de prueba.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"AN√ÅLISIS DE ERRORES ({len(error_indices)} errores total)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Mostrar algunos ejemplos de errores\n",
    "    sample_errors = np.random.choice(error_indices, \n",
    "                                   min(n_examples, len(error_indices)), \n",
    "                                   replace=False)\n",
    "    \n",
    "    for i, idx in enumerate(sample_errors):\n",
    "        true_label = y_test_labels[idx]\n",
    "        pred_label = y_pred[idx]\n",
    "        text = test_df.iloc[idx][text_column]\n",
    "        \n",
    "        print(f\"Error #{i+1}:\")\n",
    "        print(f\"Texto: {text[:150]}...\")\n",
    "        print(f\"Etiqueta real: {true_label + 1} estrellas\")\n",
    "        print(f\"Predicci√≥n: {pred_label + 1} estrellas\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "# Realizar an√°lisis de errores\n",
    "analyze_errors(X_test, y_test, y_pred, test_df, TEXT_COLUMN, TARGET_COLUMN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f120d4e",
   "metadata": {},
   "source": [
    "## 14. Conclusiones y Pr√≥ximos Pasos\n",
    "\n",
    "### Resultados obtenidos:\n",
    "- **Modelo implementado:** Perceptr√≥n Multicapa (MLP) para clasificaci√≥n de sentimientos\n",
    "- **Arquitectura:** Embedding + GlobalAveragePooling + Capas densas + Dropout\n",
    "- **Dataset:** Reviews de Amazon con clasificaci√≥n de 1-5 estrellas\n",
    "- **M√©tricas principales:** Accuracy, F1-Score, Precision, Recall\n",
    "\n",
    "### Experimentos sugeridos:\n",
    "1. **Ajuste de hiperpar√°metros:**\n",
    "   - Probar diferentes dimensiones de embedding (50, 100, 200, 300)\n",
    "   - Experimentar con diferentes arquitecturas de capas ocultas\n",
    "   - Ajustar tasas de dropout y learning rate\n",
    "\n",
    "2. **Mejoras del modelo:**\n",
    "   - Implementar t√©cnicas de regularizaci√≥n adicionales\n",
    "   - Probar diferentes optimizadores (SGD, RMSprop, AdaGrad)\n",
    "   - Experimentar con funciones de activaci√≥n alternativas\n",
    "\n",
    "3. **Preprocesamiento:**\n",
    "   - Ajustar el tama√±o del vocabulario\n",
    "   - Experimentar con diferentes longitudes de secuencia\n",
    "   - Implementar t√©cnicas de limpieza de texto m√°s sofisticadas\n",
    "\n",
    "### Archivos generados:\n",
    "- `models/`: Modelos entrenados guardados\n",
    "- `output/experiment_history.json`: Historial completo de experimentos\n",
    "- `output/`: Gr√°ficos de entrenamiento y matrices de confusi√≥n\n",
    "\n",
    "### Para la siguiente entrega:\n",
    "- Reutilizar las funciones del archivo `helper.py`\n",
    "- Comparar resultados con otras arquitecturas (RNN, CNN)\n",
    "- Implementar t√©cnicas de ensemble o voting\n",
    "- Realizar an√°lisis m√°s profundo de los errores del modelo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310ia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}